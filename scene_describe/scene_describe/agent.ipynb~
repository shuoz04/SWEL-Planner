{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入必要的库\n",
    "# 导入基础库\n",
    "import numpy as np\n",
    "import tqdm\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "from collections import namedtuple\n",
    "import sys\n",
    "# 导入pytorch库\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "# 导入绘图库\n",
    "import matplotlib.pyplot as plt\n",
    "import pyvista as pv\n",
    "pv.set_jupyter_backend('client')\n",
    "# # 导入julia\n",
    "# import julia\n",
    "# from julia import Main as env\n",
    "# env.include(\"env_cut_sim.jl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 定义网络常数\n",
    "LOG_STD_MAX = 2     # 最大标准差\n",
    "LOG_STD_MIN = -20   # 最小标准差\n",
    "\n",
    "## 自定义特征提取器\n",
    "class StateFeatureExtractor(nn.Module):\n",
    "    '''\n",
    "    特征提取器, 从state中提取特征向量, 并作为动作价值函数以及策略函数的输入\n",
    "    out_channels: 输出通道数\n",
    "    gm_size: global_map的形状。gm_size = [通道数，维度，维度，维度]\n",
    "    lm_size: local_map的形状。lm_size = [通道数，维度，维度]\n",
    "    cs_size: cs的形状(cs是一个向量)。cs_size = 维度\n",
    "    gm_output_dim: self.gm_extractor的输出维度\n",
    "    '''\n",
    "    def __init__(self, gm_out_channels, lm_out_channels, gm_size, lm_size, cs_size, gm_output_dim = 128, lm_output_dim = 512, cs_output_dim = 16, output_dim = 512):\n",
    "        super(StateFeatureExtractor, self).__init__()\n",
    "\n",
    "        self.gm_in_channels = gm_size[0]    # global_map通道数\n",
    "        self.lm_in_channels = lm_size[0]    # local_map通道数\n",
    "        self.gm_volume = gm_size[1] * gm_size[2] * gm_size[3]   # global_map通道维度之外的三个维度的形状\n",
    "        self.lm_area = lm_size[1] * lm_size[2]  # local_map通道维度之外的两个维度的形状\n",
    "        self.gm_out_channels = gm_out_channels    # global_map输出通道数\n",
    "        self.lm_out_channels = lm_out_channels  # local_map输出通道数\n",
    "        self.cs_dim = cs_size       # cs(刀具位置和姿态)的维度\n",
    "        self.gm_output_dim = gm_output_dim  # self.gm_extractor的输出维度\n",
    "        self.lm_output_dim = lm_output_dim  # self.lm_extractor的输出维度\n",
    "        self.cs_output_dim = cs_output_dim  # self.cs_extractor的输出维度\n",
    "        self.output_dim = output_dim        # self.collective_extractor的输出维度\n",
    "\n",
    "        #! 定义global_map特征提取卷积层\n",
    "        self.gm_extractor = nn.Sequential(\n",
    "            nn.Conv3d(in_channels=self.gm_in_channels, out_channels=self.gm_out_channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv3d(in_channels=self.gm_out_channels, out_channels=self.gm_out_channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(self.gm_out_channels * self.gm_volume, self.gm_output_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        #! 定义local_map特征提取卷积层\n",
    "        self.lm_extractor = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=self.lm_in_channels, out_channels=self.lm_out_channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=self.lm_out_channels, out_channels=self.lm_out_channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(self.lm_out_channels * self.lm_area, self.lm_output_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        #! 定义cs特征提取层\n",
    "        self.cs_extractor = nn.Sequential(\n",
    "            nn.Linear(self.cs_dim, self.cs_output_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        #! 整合三个特征提取层\n",
    "        self.collective_extractor = nn.Sequential(\n",
    "            nn.Linear(self.gm_output_dim + self.lm_output_dim + self.cs_output_dim, self.output_dim),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        gm = x[0]\n",
    "        lm = x[1]\n",
    "        cs = x[2]\n",
    "        gm = self.gm_extractor(gm)\n",
    "        lm = self.lm_extractor(lm)\n",
    "        cs = self.cs_extractor(cs)\n",
    "        x = torch.cat((gm, lm, cs), dim=1)\n",
    "        x = self.collective_extractor(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "## 自定义动作价值函数\n",
    "class QNetWork(nn.Module):\n",
    "    '''\n",
    "    动作价值函数, 根据状态(state)输入和动作(action)输入, 输出动作价值\n",
    "    if_feature_extractor: 是否使用特征提取器。True: 使用特征提取器, False: 不使用特征提取器。默认不使用特征提取器\n",
    "    state_dim: 状态维度。如果采用特征提取器, 则特征提取器的输出维度output_dim=state_dim\n",
    "    action_dim: 动作维度。action_dim = [x位置变化, y位置变化, z位置变化, theta角度变化, phi角度变化, 主轴转速的变化, 进给速度的变化]\n",
    "    output_dim: 输出维度, 固定为1, 输出奖励值\n",
    "    feature_dim1: 第一个隐藏层维度\n",
    "    feature_dim2: 第二个隐藏层维度\n",
    "    '''\n",
    "    def __init__(self, if_feature_extractor = True, features_extractor: StateFeatureExtractor = None, state_dim = 512, action_dim = 7, output_dim = 1, hidden_dim1 = 256, hidden_dim2 = 128):\n",
    "        super(QNetWork, self).__init__()\n",
    "\n",
    "        self.if_feature_extractor = if_feature_extractor\n",
    "        self.features_extractor = features_extractor\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim1 = hidden_dim1\n",
    "        self.hidden_dim2 = hidden_dim2\n",
    "\n",
    "        #! Q值网络\n",
    "        self.q_network = nn.Sequential(\n",
    "            nn.Linear(self.state_dim + self.action_dim, self.hidden_dim1),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_dim1, self.hidden_dim2),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_dim2, self.output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, state, action):\n",
    "        if self.if_feature_extractor:   # 采用特征提取器, 将state转换成特征向量state\n",
    "            #* 动作价值函数与策略函数共用一个特征提取器\n",
    "            with torch.no_grad():\n",
    "                state = self.features_extractor(state)\n",
    "        x = torch.cat([state, action], dim=1)\n",
    "        x = self.q_network(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "## 自定义策略函数\n",
    "class PolicyNetWork(nn.Module):\n",
    "    '''\n",
    "    策略函数, 根据状态(state)输入, 输出动作(action)概率\n",
    "    if_feature_extractor: 是否使用特征提取器。True: 使用特征提取器, False: 不使用特征提取器。默认不使用特征提取器\n",
    "    state_dim: 状态维度\n",
    "    action_dim: 动作维度\n",
    "    feature_dim1: 第一个隐藏层维度\n",
    "    feature_dim2: 第二个隐藏层维度\n",
    "    '''\n",
    "    def __init__(self, if_feature_extractor = True, features_extractor: StateFeatureExtractor = None, state_dim = 512, action_dim = 7, hidden_dim = 256, output_dim = 128):\n",
    "        super(PolicyNetWork, self).__init__()\n",
    "\n",
    "        self.if_feature_extractor = if_feature_extractor\n",
    "        self.features_extractor = features_extractor\n",
    "        self.state_dim = state_dim\n",
    "        self.action_dim = action_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.output_dim = output_dim\n",
    "\n",
    "        #! 策略网络\n",
    "        self.policy_dist = nn.Sequential(\n",
    "            nn.Linear(self.state_dim, self.hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(self.hidden_dim, self.output_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        #! 输出均值和标准差\n",
    "        # TODO：https://arxiv.org/abs/2005.05719 提出一种根据状态的采样策略，可以尝试使用\n",
    "        self.log_std = nn.Linear(self.output_dim, self.action_dim)\n",
    "        self.mu = nn.Linear(self.output_dim, self.action_dim)\n",
    "\n",
    "    def forward(self, state, deterministic = False):\n",
    "        '''\n",
    "        前向传播\n",
    "        state: 状态输入\n",
    "        deterministic: 是否确定性输出，True: 输出均值，False: 输出均值和标准差的采样结果\n",
    "        '''\n",
    "        if self.if_feature_extractor:   # 使用特征提取器, 将state转换成特征向量state\n",
    "            #* 动作价值函数与策略函数共用一个特征提取器, 并且特征提取器在\n",
    "            state = self.features_extractor(state)\n",
    "        x = self.policy_dist(state)\n",
    "        mu = self.mu(x)\n",
    "        if deterministic:\n",
    "            return mu\n",
    "        else:\n",
    "            log_std = self.log_std(x)\n",
    "            log_std = torch.clamp(log_std, LOG_STD_MIN, LOG_STD_MAX)\n",
    "            std = torch.exp(log_std)\n",
    "            dist = torch.distributions.Normal(mu, std)\n",
    "            return dist.rsample()\n",
    "        \n",
    "    def mu_and_log_std(self, state, deterministic = False):\n",
    "        '''\n",
    "        输出均值和标准差，用于policy的训练\n",
    "        state: 状态输入\n",
    "        '''\n",
    "        # 0. 特征提取器提取特征\n",
    "        if self.if_feature_extractor:\n",
    "            state = self.features_extractor(state)\n",
    "        # 1. 通过policy网络得到log_std和mu\n",
    "        x = self.policy_dist(state)\n",
    "        mu = self.mu(x)\n",
    "        log_std = self.log_std(x)\n",
    "        log_std = torch.clamp(log_std, LOG_STD_MIN, LOG_STD_MAX)    # 限制log_std的范围\n",
    "        std = torch.exp(log_std)    # 得到真正的标准差\n",
    "        # 2. 通过std和mu创建高斯分布\n",
    "        dist = torch.distributions.Normal(mu, std)\n",
    "        # 3. 返回从高斯分布中采样的动作\n",
    "        action = dist.rsample()\n",
    "        # 4. 计算动作分布的标准差并返回对数标准差的和\n",
    "        log_prob = dist.log_prob(action)\n",
    "        log_prob -= torch.log(1 - action ** 2 + 1e-6)\n",
    "        log_prob = log_prob.sum(1, keepdim=True)\n",
    "        # 5. 将动作限制在[-1, 1]之间\n",
    "        action = torch.tanh(action)\n",
    "        return action, log_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 经验回放缓冲区\n",
    "# TODO: 优化经验回放缓冲区\n",
    "Transition = namedtuple('Transition',\n",
    "                        ('state','reward', 'done', 'action', 'next_state'))\n",
    "\n",
    "#// class Memory:\n",
    "#//     def __init__(self, memory_size):\n",
    "#//         self.memory_size = memory_size\n",
    "#//         self.memory = []\n",
    "#//     def add(self, *transition):\n",
    "#//         self.memory.append(Transition(*transition))\n",
    "#//         if len(self.memory) > self.memory_size:\n",
    "#//             self.memory.pop(0)\n",
    "#//         assert len(self.memory) <= self.memory_size\n",
    "#//     def sample(self, batch_size = 256):\n",
    "#//         return random.sample(self.memory, batch_size)\n",
    "#//     def __len__(self):\n",
    "#//         return len(self.memory)\n",
    "\n",
    "class ReplayBuffer:\n",
    "    '''\n",
    "    经验回放缓冲区\n",
    "    memory_size: 缓冲区大小\n",
    "    batch_size: 批次大小\n",
    "    '''\n",
    "    def __init__(self, memory_size = 1000000, batch_size = 256):\n",
    "        self.memory_size = memory_size\n",
    "        self.batch_size = batch_size\n",
    "        self.memory = []\n",
    "\n",
    "    def add(self, state, reward, done, action, next_state):\n",
    "        self.memory.append(Transition(state, reward, done, action, next_state))\n",
    "        if len(self.memory) > self.memory_size:\n",
    "            self.memory.pop(0)\n",
    "        assert len(self.memory) <= self.memory_size\n",
    "\n",
    "    def sample(self):\n",
    "        return random.sample(self.memory, self.batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SAC算法\n",
    "class SAC:\n",
    "    '''\n",
    "    learning_rate: 学习率\n",
    "    tau: 软更新参数\n",
    "    gamma: 折扣因子\n",
    "    buffer_size: 经验回放缓冲区大小\n",
    "    batch_size: 训练batch大小\n",
    "    if_feature_extractor: 是否使用特征提取器\n",
    "    gm_out_channels: global_map输出通道数\n",
    "    lm_out_channels: local_map输出通道数\n",
    "    gm_size: global_map的形状。gm_size = [通道数，维度，维度，维度]\n",
    "    lm_size: local_map的形状。lm_size = [通道数，维度，维度]\n",
    "    cs_size: cs的形状(cs是一个向量)。cs_size = 维度\n",
    "    gm_output_dim: self.gm_extractor的输出维度\n",
    "    lm_output_dim: self.lm_extractor的输出维度\n",
    "    cs_output_dim: self.cs_extractor的输出维度\n",
    "    features_dim: 特征提取器的输出维度\n",
    "    action_dim: 动作维度\n",
    "    q_hidden_dim1: Q网络第一个隐藏层维度\n",
    "    q_hidden_dim2: Q网络第二个隐藏层维度\n",
    "    p_hidden_dim1: 策略网络第一个隐藏层维度\n",
    "    p_hidden_dim2: 策略网络第二个隐藏层维度\n",
    "    ent_coef: 熵系数\n",
    "    '''\n",
    "    def __init__(\n",
    "            self,\n",
    "            env,\n",
    "            learning_rate = 0.003,\n",
    "            tau = 0.005,\n",
    "            gamma = 0.99,\n",
    "            buffer_size = 1000000,\n",
    "            batch_size = 256,\n",
    "            if_feature_extractor = False,\n",
    "            gm_out_channels = 2,\n",
    "            lm_out_channels = 18,\n",
    "            gm_size = [2, 10, 10, 10],\n",
    "            lm_size = [18, 75, 75],\n",
    "            cs_size = 9,\n",
    "            gm_output_dim = 128,\n",
    "            lm_output_dim = 512,\n",
    "            cs_output_dim = 16,\n",
    "            features_dim = 512,\n",
    "            action_dim = 7,\n",
    "            q_hidden_dim1 = 256,\n",
    "            q_hidden_dim2 = 128,\n",
    "            p_hidden_dim1 = 256,\n",
    "            p_hidden_dim2 = 128,\n",
    "            ent_coef = 0.1\n",
    "    ):\n",
    "        self.env = env\n",
    "        self.learning_rate = learning_rate\n",
    "        self.tau = tau\n",
    "        self.gamma = gamma\n",
    "        self.buffer_size = buffer_size\n",
    "        self.batch_size = batch_size\n",
    "        self.if_feature_extractor = if_feature_extractor\n",
    "        self.features_dim = features_dim    # 特征提取器的输出维度，同时也是Q网络和策略网络的状态输入维度\n",
    "        self.action_dim = action_dim        # 动作维度\n",
    "        self.q_hidden_dim1 = q_hidden_dim1  # Q网络第一个隐藏层维度\n",
    "        self.q_hidden_dim2 = q_hidden_dim2\n",
    "        self.p_hidden_dim1 = p_hidden_dim1\n",
    "        self.p_hidden_dim2 = p_hidden_dim2\n",
    "        self.target_entropy = -self.action_dim  # 目标熵\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        # 使用特征提取器\n",
    "        if if_feature_extractor:\n",
    "            self.gm_out_channels = gm_out_channels\n",
    "            self.lm_out_channels = lm_out_channels\n",
    "            self.gm_size = gm_size\n",
    "            self.lm_size = lm_size\n",
    "            self.cs_size = cs_size\n",
    "            self.gm_output_dim = gm_output_dim\n",
    "            self.lm_output_dim = lm_output_dim\n",
    "            self.cs_output_dim = cs_output_dim\n",
    "            self.features_extractor = StateFeatureExtractor(gm_output_channels=self.gm_out_channels, lm_out_channels=self.lm_out_channels, \n",
    "                                                            gm_size=self.gm_size, lm_size=self.lm_size, cs_size=self.cs_size, \n",
    "                                                            gm_output_dim=self.gm_output_dim, lm_output_dim=self.lm_output_dim, cs_output_dim=self.cs_output_dim, output_dim=self.features_dim).to(self.device)\n",
    "        else:\n",
    "            self.features_extractor = None\n",
    "\n",
    "        # 定义Q网络\n",
    "        self.q1 = QNetWork(if_feature_extractor=self.if_feature_extractor, features_extractor=self.features_extractor, \n",
    "                      state_dim=self.features_dim, action_dim=self.action_dim, hidden_dim1=self.q_hidden_dim1, hidden_dim2=self.q_hidden_dim2).to(self.device)\n",
    "        self.q2 = QNetWork(if_feature_extractor=self.if_feature_extractor, features_extractor=self.features_extractor,\n",
    "                      state_dim=self.features_dim, action_dim=self.action_dim, hidden_dim1=self.q_hidden_dim1, hidden_dim2=self.q_hidden_dim2).to(self.device)\n",
    "        self.q1_target = QNetWork(if_feature_extractor=self.if_feature_extractor, features_extractor=self.features_extractor,\n",
    "                             state_dim=self.features_dim, action_dim=self.action_dim, hidden_dim1=self.q_hidden_dim1, hidden_dim2=self.q_hidden_dim2).to(self.device)\n",
    "        self.q2_target = QNetWork(if_feature_extractor=self.if_feature_extractor, features_extractor=self.features_extractor,\n",
    "                             state_dim=self.features_dim, action_dim=self.action_dim, hidden_dim1=self.q_hidden_dim1, hidden_dim2=self.q_hidden_dim2).to(self.device)\n",
    "        self.q1_opt = optim.Adam(self.q1.parameters(), lr=self.learning_rate)   # 定义Q网络优化器\n",
    "        self.q2_opt = optim.Adam(self.q2.parameters(), lr=self.learning_rate)   # 定义Q网络优化器\n",
    "\n",
    "        # 定义策略网络\n",
    "        self.policy_net = PolicyNetWork(state_dim=self.features_dim, action_dim=self.action_dim, \n",
    "                                        if_feature_extractor=self.if_feature_extractor, \n",
    "                                        hidden_dim=self.p_hidden_dim1, output_dim=self.p_hidden_dim2).to(self.device)\n",
    "        if self.if_feature_extractor:   # 采用特征提取器, 则策略网络和特征提取器共同训练\n",
    "            self.policy_opt = optim.Adam(list(self.features_extractor.parameters() + list(self.policy_net.parameters())), lr=self.learning_rate)\n",
    "        else:\n",
    "            self.policy_opt = optim.Adam(self.policy_net.parameters(), lr=self.learning_rate)   # 定义策略网络优化器\n",
    "        \n",
    "        # 定义经验回放缓冲区\n",
    "        self.replay_buffer = ReplayBuffer(self.buffer_size, self.batch_size)\n",
    "\n",
    "        # 定义温度\n",
    "        self.log_ent_coef = torch.tensor([0.0]).to(self.device).requires_grad_()\n",
    "        self.ent_coef_opt = optim.Adam([self.log_ent_coef], lr=self.learning_rate)\n",
    "\n",
    "    def choose_action(self, state, deterministic = True):\n",
    "        state = torch.FloatTensor(state).to(self.device)\n",
    "        action = self.policy_net(state, deterministic = deterministic)\n",
    "        return action.cpu().detach().numpy()\n",
    "\n",
    "    def store(self, state, reward, done, action, next_state):\n",
    "        state = torch.FloatTensor(state)\n",
    "        reward = torch.Tensor([reward])\n",
    "        done = torch.Tensor([done])\n",
    "        action = np.array([action])\n",
    "        action = torch.Tensor(action)\n",
    "        next_state = torch.FloatTensor(next_state)\n",
    "        self.replay_buffer.add(state, reward, done, action, next_state)\n",
    "\n",
    "    def unpack(self, batch):\n",
    "        batch = Transition(*zip(*batch))\n",
    "        states = torch.cat(batch.state).view(self.batch_size, self.features_dim).to(self.device)\n",
    "        rewards = torch.cat(batch.reward).view(self.batch_size, 1).to(self.device)\n",
    "        dones = torch.cat(batch.done).view(self.batch_size, 1).to(self.device)\n",
    "        actions = torch.cat(batch.action).view(-1, self.action_dim).to(self.device)\n",
    "        next_states = torch.cat(batch.next_state).view(self.batch_size, self.features_dim).to(self.device)\n",
    "        return states, rewards, dones, actions, next_states\n",
    "\n",
    "    def train(self):\n",
    "        '''\n",
    "        训练函数\n",
    "        '''\n",
    "        # 0. 经验回放缓冲区中取样\n",
    "        if len(self.replay_buffer) < self.batch_size:\n",
    "            return\n",
    "        batch = self.replay_buffer.sample()\n",
    "        state, reward, done, action, next_state = self.unpack(batch)\n",
    "\n",
    "        # 1. 策略网络根据state预测动作\n",
    "        action, log_prob = self.policy_net.mu_and_log_std(state)\n",
    "        # log_prob = log_prob.reshape(-1, 1) \n",
    "\n",
    "        # 2. 更新温度\n",
    "        ent_coef = torch.exp(self.log_ent_coef.detach())\n",
    "        ent_coef_loss = -(self.log_ent_coef * (log_prob + self.target_entropy).detach()).mean()\n",
    "        self.ent_coef_opt.zero_grad()\n",
    "        ent_coef_loss.backward()\n",
    "        self.ent_coef_opt.step()\n",
    "         \n",
    "        # 3. 更新状态价值函数\n",
    "        # 3.1 计算目标Q值target\n",
    "        with torch.no_grad():   # 不更新q1_target和q2_target\n",
    "            next_action, next_log_prob = self.policy_net.mu_and_log_std(next_state)\n",
    "            q1_target = self.q1_target(next_state, next_action)\n",
    "            q2_target = self.q2_target(next_state, next_action)\n",
    "            q_target = torch.min(q1_target, q2_target)\n",
    "            #! 这里next_log_prob的维度可能出现问题\n",
    "            target_q = reward + self.gamma * (1 - done) * (q_target - ent_coef * next_log_prob)\n",
    "\n",
    "        # 3.2 计算q网络的当前q值current_q1, current_q2\n",
    "        current_q1 = self.q1(state, action)\n",
    "        current_q2 = self.q2(state, action)\n",
    "\n",
    "        # 3.3 计算q网络的损失函数\n",
    "        q1_loss = F.mse_loss(current_q1, target_q)\n",
    "        q2_loss = F.mse_loss(current_q2, target_q)\n",
    "\n",
    "        # 3.4 更新q网络\n",
    "        self.q1_opt.zero_grad()\n",
    "        q1_loss.backward(retain_graph=True)\n",
    "        self.q1_opt.step()\n",
    "        self.q2_opt.zero_grad()\n",
    "        q2_loss.backward(retain_graph=True)\n",
    "        self.q2_opt.step()\n",
    "\n",
    "        # 4. 更新策略网络\n",
    "        # 4.1 计算策略网络的损失函数\n",
    "        q1 = self.q1(state, action)\n",
    "        q2 = self.q2(state, action)\n",
    "        q_min = torch.min(q1, q2)\n",
    "        policy_loss = (ent_coef * log_prob - q_min).mean()\n",
    "\n",
    "        # 4.2 更新策略网络\n",
    "        self.policy_opt.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        self.policy_opt.step()\n",
    "\n",
    "        # 5. 软更新目标Q网络\n",
    "        for param, target_param in zip(self.q1.parameters(), self.q1_target.parameters()):\n",
    "            target_param.data.copy_(self.tau * param.data + (1 - self.tau) * target_param.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 工具类，用于显示渲染结果\n",
    "from IPython import display # 导入display模块，用于在Jupyter notebook中显示图像\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class GymHelper:\n",
    "    def __init__(self, env, figsize=(3,3)):\n",
    "        self.env = env\n",
    "        self.figsize = figsize\n",
    "        plt.figure(figsize = figsize)\n",
    "        self.img = plt.imshow(env.render())\n",
    "\n",
    "    def render(self, title = None):\n",
    "        image_data = self.env.render()\n",
    "        self.img.set_data(image_data)   # 更新绘图窗口中的图像数据\n",
    "        display.display(plt.gcf())      # 刷新显示\n",
    "        display.clear_output(wait = True)   #有新图像时再清楚绘图窗口原有的图像\n",
    "        if title:   # 如果有标题，就显示标题\n",
    "            plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## 测试\n",
    "# 导入gym环境\n",
    "import gymnasium as gym\n",
    "\n",
    "# 创建CartPole环境，指定渲染模式为rgb_array，如果是在IDE中可以改为“human”\n",
    "env = gym.make('LunarLanderContinuous-v2', render_mode='rgb_array', continuous = True)\n",
    "# 重置环境\n",
    "env.reset()\n",
    "# 创建GymHelper对象\n",
    "gym_helper = GymHelper(env)\n",
    "\n",
    "# 循环N次\n",
    "N = 100\n",
    "for i in range(N):\n",
    "    gym_helper.render(title = str(i))    # 渲染环境\n",
    "    action = env.action_space.sample()  #从动作空间中随机选取一个动作\n",
    "    env.step(action)    # 执行这个动作\n",
    "\n",
    "# 关闭环境\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Dimension: 2\n",
      "State Dimension: 8\n"
     ]
    }
   ],
   "source": [
    "action_dim = env.action_space.shape[0]\n",
    "state_dim = env.observation_space.shape[0]\n",
    "\n",
    "print(f\"Action Dimension: {action_dim}\")\n",
    "print(f\"State Dimension: {state_dim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/500 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Dimension out of range (expected to be in range of [-1, 0], but got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# 循环惊醒每一步的操作\u001b[39;00m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_steps):\n\u001b[0;32m     18\u001b[0m     \u001b[38;5;66;03m# 根据当前状态选择动作\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoose_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m     next_state, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)  \u001b[38;5;66;03m# 与环境交互产生下一个状态并生成奖励等相关信息\u001b[39;00m\n\u001b[0;32m     21\u001b[0m     done \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n",
      "Cell \u001b[1;32mIn[16], line 113\u001b[0m, in \u001b[0;36mSAC.choose_action\u001b[1;34m(self, state, deterministic)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mchoose_action\u001b[39m(\u001b[38;5;28mself\u001b[39m, state, deterministic \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    112\u001b[0m     state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(state)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m--> 113\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdeterministic\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m action\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n",
      "File \u001b[1;32mc:\\applications\\anaconda\\envs\\cwe\\lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\applications\\anaconda\\envs\\cwe\\lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[9], line 191\u001b[0m, in \u001b[0;36mPolicyNetWork.forward\u001b[1;34m(self, state, deterministic)\u001b[0m\n\u001b[0;32m    189\u001b[0m log_prob \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39mlog_prob(action)\n\u001b[0;32m    190\u001b[0m log_prob \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m action \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1e-6\u001b[39m)\n\u001b[1;32m--> 191\u001b[0m log_prob \u001b[38;5;241m=\u001b[39m \u001b[43mlog_prob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m    192\u001b[0m \u001b[38;5;66;03m# 5. 将动作限制在[-1, 1]之间\u001b[39;00m\n\u001b[0;32m    193\u001b[0m action \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtanh(action)\n",
      "\u001b[1;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"
     ]
    }
   ],
   "source": [
    "# 定义参数\n",
    "max_episodes = 500 # 迭代训练次数\n",
    "max_steps = 1000    # 每个回合的最大步数\n",
    "\n",
    "# 创建对象\n",
    "agent = SAC(env, features_dim=state_dim, action_dim=action_dim)\n",
    "episode_rewards = []    # 只是用来看学习的成果的，和训练过程无关\n",
    "\n",
    "# 开始迭代训练\n",
    "for episode in tqdm(range(max_episodes), file=sys.stdout):\n",
    "    # 重置环境并获取初始状态\n",
    "    state, _ = env.reset()\n",
    "    # 初始化当前回合的奖励\n",
    "    episode_reward = 0\n",
    "\n",
    "    # 循环惊醒每一步的操作\n",
    "    for step in range(max_steps):\n",
    "        # 根据当前状态选择动作\n",
    "        action = agent.choose_action(state)\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)  # 与环境交互产生下一个状态并生成奖励等相关信息\n",
    "        done = terminated or truncated\n",
    "        # 存入经验回放缓冲区\n",
    "        agent.store(state, reward, done, action, next_state)\n",
    "        agent.train()\n",
    "        episode_reward += reward\n",
    "\n",
    "        # 更新当前状态\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "    # 记录当前回合的奖励\n",
    "    episode_rewards.append(episode_reward)\n",
    "\n",
    "    if episode % 10 == 0:\n",
    "        tqdm.write(\"Episode\"+str(episode)+\":\"+str(episode_reward))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 300x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 重置环境开始新一轮的游戏\n",
    "observation, _ = env.reset()\n",
    "# 创建GymHelper对象\n",
    "gym_helper = GymHelper(env, figsize = (3,3))\n",
    "\n",
    "# 循环N次\n",
    "N = 300\n",
    "for i in range(N):\n",
    "    gym_helper.render(title = str(i))    # 渲染环境\n",
    "    action = agent.choose_action(observation, True)\n",
    "    observation, reward, termintated, truncated, info = env.step(action)\n",
    "\n",
    "# 关闭环境\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cwe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
